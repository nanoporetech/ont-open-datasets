<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Oxford Nanopore Technologies Open Datasets]]></title><description><![CDATA[Discover Open Datasets provided by Oxford Nanopore Technologies.]]></description><link>https://nanoporetech.github.io/ont-open-datasets</link><generator>GatsbyJS</generator><lastBuildDate>Fri, 30 Oct 2020 10:07:38 GMT</lastBuildDate><item><title><![CDATA[Bonito basecalling with R9.4.1]]></title><description><![CDATA[Updated 2020-10-30: This page was edited to reflect the formal release of Bonito v0.3.0 We are please to announce the addition of
bonito…]]></description><link>https://nanoporetech.github.io/ont-open-datasets/bonito/</link><guid isPermaLink="false">https://nanoporetech.github.io/ont-open-datasets/bonito/</guid><pubDate>Thu, 15 Oct 2020 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;&lt;strong&gt;Updated 2020-10-30: This page was edited to reflect the formal release of Bonito v0.3.0&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We are please to announce the addition of
&lt;a href=&quot;https://github.com/nanoporetech/bonito&quot;&gt;bonito&lt;/a&gt; basecalling results to the
GM24385 dataset. Bonito is a research-grade, open source basecaller utilising
the &lt;a href=&quot;https://pytorch.org/&quot;&gt;PyTorch&lt;/a&gt; library; its development explores
alternative basecalling frameworks to those use in the product-grade Guppy
basecalling software.&lt;/p&gt;
&lt;p&gt;The Bonito basecalling for the GM24385 dataset was performed using version
0.3.0, driven by the same &lt;a href=&quot;/ont-open-datasets/katuali_human_pipeline/&quot;&gt;katuali&lt;/a&gt; analysis pipeline
as for the initial dataset release.  The Bonito basecaller was provided as
input the per-chromosome &lt;code class=&quot;language-text&quot;&gt;.fast5&lt;/code&gt; files created in the initial pipeline via
alignment of the Guppy 4.0.11 basecalls. This allows for easy comparison of
results on subsets of the data (but may lead to subtle side-effects). For
example the analysis data structure contains now entries of the form:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;gm24385_2020.09/analysis/r9.4.1/{flowcell}/guppy_{suffix}/align_unfiltered/{chromosome}/bonito_v0.3.0/
├── align_unfiltered
│   ├── align_to_ref.log
│   ├── basecall_stats.log
│   ├── calls2ref.bam
│   ├── calls2ref.bam.bai
│   └── calls2ref_stats.txt
├── basecalls.fastq.gz
└── basecalls.fastq.gz_summary.tsv&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The file &lt;code class=&quot;language-text&quot;&gt;basecalls.fastq.gz&lt;/code&gt; contains the basecalling results from Bonito. The
quality scores in these files have been mocked as the pre-release build of Bonito used
does not yet provide quality scores. Similar to the main folder structure the
&lt;code class=&quot;language-text&quot;&gt;align_unfiltered&lt;/code&gt; directory contains unfiltered alignments of the basecalls to
the reference sequence (&lt;code class=&quot;language-text&quot;&gt;calls2ref.bam&lt;/code&gt;) along with text files summarizing the
properties of the alignments.&lt;/p&gt;
&lt;h3&gt;Comparison with Guppy 4.0.11 basecalls&lt;/h3&gt;
&lt;p&gt;As a basis for comparison with the current Guppy basecaller we can use the alignment summary
files for both the Guppy and Bonito basecalls. To simplify the analysis we compare only chromosome 1
data for a single flowcell; we can download the files with:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;aws s3 cp --no-sign-request s3://ont-open-data/gm24385_2020.09/analysis/r9.4.1/20200914_1354_6B_PAF27096_e7c9eae6/guppy_v4.0.11_r9.4.1_hac_prom/align_unfiltered/chr1/calls2ref_stats.txt guppy.stats
aws s3 cp --no-sign-request s3://ont-open-data/gm24385_2020.09/analysis/r9.4.1/20200914_1354_6B_PAF27096_e7c9eae6/guppy_v4.0.11_r9.4.1_hac_prom/align_unfiltered/chr1/bonito_v0.3.0/align_unfiltered/calls2ref_stats.txt bonito.stats&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The following &lt;a href=&quot;/ont-open-datasets/c0d85ae03e203a7f7e5cdaab853f6819/plot.py&quot;&gt;python code&lt;/a&gt;,&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;from concurrent.futures import ProcessPoolExecutor

import pandas as pd

import aplanat.util
from aplanat import lines
from aplanat.layouts import facet_grid
from aplanat.report import HTMLReport


def read_data(args):
    caller, filename = args
    df = pd.read_csv(filename, sep=&amp;#39;\t&amp;#39;)
    xs, ys = aplanat.util.kernel_density_estimate(df[&amp;#39;acc&amp;#39;], step=0.05)
    df = pd.DataFrame({&amp;#39;accuracy&amp;#39;:xs, &amp;#39;density&amp;#39;:ys})
    df[&amp;#39;caller&amp;#39;] = caller
    return df

data_sets = {
    &amp;#39;bonito&amp;#39;: &amp;#39;bonito.stats&amp;#39;,
    &amp;#39;guppy&amp;#39;: &amp;#39;guppy.stats&amp;#39;}

with ProcessPoolExecutor() as executor:
    report = HTMLReport(&amp;quot;Basecalling Accuracy Summary&amp;quot;, &amp;quot;GM24385&amp;quot;)
    dfs = list(executor.map(read_data, data_sets.items()))
plot = lines.line(
    [df[&amp;#39;accuracy&amp;#39;] for df in dfs],
    [df[&amp;#39;density&amp;#39;] for df in dfs],
    colors=[&amp;#39;red&amp;#39;, &amp;#39;blue&amp;#39;],
    names=[&amp;#39;bonito&amp;#39;, &amp;#39;guppy&amp;#39;],
    xlim=(85,100),
    x_axis_label=&amp;#39;Alignment accuracy&amp;#39;,
    y_axis_label=&amp;#39;Density&amp;#39;)
plot.legend.location = &amp;#39;top_left&amp;#39;
report.plot(plot)
report.write(&amp;quot;report.html&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;can be used to plot a kernel density estimate for the read alignment accuracy:&lt;/p&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/ont-open-datasets/static/53bb1ce70c47b4b55e01304c2e3b79e1/0a47e/accuracy.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 50%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsSAAALEgHS3X78AAABiElEQVQoz4VSi3LcIAz0/39jm8y056Rnx08eBgHaLDhOe9PMBI8GYYnVakUHwBtrU4pRchKRlKSU8r1pEa254yglZ55VlNZViHmZEUxA/6vH+DaiLgab/ev//afIBSjOQ/cdBP7M67jLNE0gP7XOq/der0UmD/vl1+MyZzV3R54kZm0tU4PaWWtlXVeQ6UmHi5datf/3mlJgHFnYA24+IDEDzqLxZl53u91k27YGeF3+CvAEU4RAQEvXbmAzCE4eAauQF8OLxQMgzzWxgqWs2Aw5WgIcnmfAGMa8BQdzalgBl3VhMH20rO07vdK8CuRdxjYL8sYheAfKiJzO9uEMNKcTMHH04zAi+kiBC1vIcHvmW6pWmk72bhFm3nS0j4lei4NG3AOLHK14m3IMES8/er0/37H0I6bbC8ww4Y3PyMwrgXdqd+CIEbsx4CCxLAs19NxXDIPF+HtAOI6z5VopStSYIlIhMwZqS0EiNr6zn0/PeH39g77vG1DV1pFtpm4VlO8akgSRBd8BFWAN/o603t0AAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;accuracy comparison&quot;
        title=&quot;Basecalls: Bonito CTC-CRF vs. Guppy 4.0.11&quot;
        src=&quot;/ont-open-datasets/static/53bb1ce70c47b4b55e01304c2e3b79e1/fcda8/accuracy.png&quot;
        srcset=&quot;/ont-open-datasets/static/53bb1ce70c47b4b55e01304c2e3b79e1/12f09/accuracy.png 148w,
/ont-open-datasets/static/53bb1ce70c47b4b55e01304c2e3b79e1/e4a3f/accuracy.png 295w,
/ont-open-datasets/static/53bb1ce70c47b4b55e01304c2e3b79e1/fcda8/accuracy.png 590w,
/ont-open-datasets/static/53bb1ce70c47b4b55e01304c2e3b79e1/0a47e/accuracy.png 600w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The plot indicates a decrease of one-third in the modal error of reads.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Small variant calling with GM24385]]></title><description><![CDATA[In this blog post we will explore small variant calling using the previously
released HG002 (GM24385 Ashkenazi Son) data release. The GM…]]></description><link>https://nanoporetech.github.io/ont-open-datasets/gm24385_snp/</link><guid isPermaLink="false">https://nanoporetech.github.io/ont-open-datasets/gm24385_snp/</guid><pubDate>Thu, 15 Oct 2020 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;In this blog post we will explore small variant calling using the &lt;a href=&quot;/ont-open-datasets/gm24385_2020.09&quot;&gt;previously
released&lt;/a&gt; HG002 (GM24385 Ashkenazi Son) data release.&lt;/p&gt;
&lt;p&gt;The GM24385 dataset comprises whole genome sequencing of a well-characterised
human cell line. It therefore provides a useful benchmark sample; the cell line
was also used as a “seen” sample in the recent
&lt;a href=&quot;PrecisionFDA%20Truth%0AChallenge%20V2&quot;&gt;https://precision.fda.gov/challenges/10/view/results&lt;/a&gt; competition.&lt;/p&gt;
&lt;h3&gt;Variant Calling with Medaka and DeepVariant&lt;/h3&gt;
&lt;p&gt;As an easily reproducible example we will focus on chromosome 20 of the genome
rather than performing computation on the whole genome.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;This walkthrough assumes some familiarity with standard bioinformatic tools
for handling genomics data. A working installation of
&lt;a href=&quot;http://www.htslib.org/&quot;&gt;samtools&lt;/a&gt;,
&lt;a href=&quot;https://bedtools.readthedocs.io/en/latest/&quot;&gt;bedtools&lt;/a&gt;,
&lt;a href=&quot;https://github.com/nanoporetech/medaka&quot;&gt;medaka&lt;/a&gt;,
&lt;a href=&quot;https://www.docker.com/get-started&quot;&gt;docker&lt;/a&gt;, and the &lt;a href=&quot;https://aws.amazon.com/cli/&quot;&gt;AWS command-line
tools&lt;/a&gt; are required to follow the process
below.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Data preparation&lt;/h4&gt;
&lt;p&gt;To start let us download the pre-aligned reads corresponding to chromosome 20
from the dataset resource (see our &lt;a href=&quot;/ont-open-datasets/tutorials&quot;&gt;tutorial&lt;/a&gt; FAQs) for more
information on downloading data):&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;for ext in .bam .bam.bai; do
    aws s3 --no-sign-request cp s3://ont-open-data/gm24385_2020.09/analysis/r9.4.1/20200914_1354_6B_PAF27096_e7c9eae6/guppy_v4.0.11_r9.4.1_hac_prom/align_unfiltered/chr20/calls2ref${ext} PAF27096.chr20${ext}
    aws s3 --no-sign-request cp s3://ont-open-data/gm24385_2020.09/analysis/r9.4.1/20200914_1357_1-E11-H11_PAF27462_d3c9678e/guppy_v4.0.11_r9.4.1_hac_prom/align_unfiltered/chr20/calls2ref${ext} PAF27462.chr20${ext}
done&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and merge these into a single &lt;code class=&quot;language-text&quot;&gt;.bam&lt;/code&gt; file:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;samtools merge chr20.bam PAF27096.chr20.bam PAF27462.chr20.bam
samtools index chr20.bam&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As an additional step we will filter the &lt;code class=&quot;language-text&quot;&gt;chr20.bam&lt;/code&gt; file to leave only primary
alignments, removing secondary and supplementary alignments. This is necessary
as DeepVariant, which we will use later, will otherwise process these
additional alignments.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;samtools view chr20.bam -F 2308 -@ 64 -b &amp;gt; chr20.primary.bam
samtools index chr20.primary.bam&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To perform variant calling we also require the human reference sequence, the
same sequence as used to create the alignment files prepared above. An indexed
copy of this is available from the dataset resource:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;for ext in .fasta .fasta.fai; do
    aws s3 --no-sign-request cp s3://ont-open-data/gm24385_2020.09/config/ref/GCA_000001405.15_GRCh38_no_alt_analysis_set${ext} .
done&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We have now all the inputs required to perform variant calling with medaka
and DeepVariant.&lt;/p&gt;
&lt;h4&gt;Running Medaka&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/nanoporetech/medaka&quot;&gt;Medaka&lt;/a&gt; is Oxford Nanopore
Technologies’ software for performing consensus and small variant calling from
nanopore long-read data. It can perform diploid variant calling either in
isolation or in tandem with
&lt;a href=&quot;https://github.com/google/deepvariant&quot;&gt;DeepVariant&lt;/a&gt;; to enable this second
use-case small modifications have been made to how medaka represents variants
to allow it to function as a variant-candidate generator for DeepVariant.&lt;/p&gt;
&lt;p&gt;To perform candidate generation with medaka we run:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;medaka_variant -i chr20.primary.bam -f GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta \
    -r chr20 -t 8 -P 0 -l&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The final two options here (&lt;code class=&quot;language-text&quot;&gt;-P 0 -l&lt;/code&gt;) instruct medaka to leave its
substitution calls unfiltered and to decompose multi-nucleotide substitutions
into independent single nucleotide substitutions. Details of workflow employed
by the &lt;code class=&quot;language-text&quot;&gt;medaka_variant&lt;/code&gt; program can be found in the medaka
&lt;a href=&quot;https://nanoporetech.github.io/medaka/snp.html#&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The result of running the above command will be a directory &lt;code class=&quot;language-text&quot;&gt;medaka_variant&lt;/code&gt;
containing (amongst other files):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;code class=&quot;language-text&quot;&gt;round_0_hap_mixed_phased.bam&lt;/code&gt;&lt;/em&gt;: alignments (as in &lt;code class=&quot;language-text&quot;&gt;chr20.primary.bam&lt;/code&gt;), tagged with a calculated haplotype,&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;code class=&quot;language-text&quot;&gt;round_1.vcf&lt;/code&gt;&lt;/em&gt;: the final output variant candidates for DeepVariant.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Running DeepVariant&lt;/h4&gt;
&lt;p&gt;With variant-candidate generation performed by medaka we can now use
DeepVariant to calculate our final variant calls. In order to simplify this we
will download a program that automates the execution of the docker container
used to run DeepVariant:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;wget https://gist.githubusercontent.com/cjw85/23d2b0675ec5a5c7fd4074456524c971/raw/c716c85639f047b9a9cff2079be2868bccb61659/run_deepvariant.sh
chmod +x run_deepvariant.sh&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This script simply gathers together the required inputs before executing
DeepVariant within the docker container, it can be run simply with:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;./run_deepvariant.sh \
    -b medaka_variant/round_0_hap_mixed_phased.bam \
    -v medaka_variant/round_1.vcf \
    -r GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta \
    -o deepvariant -t 64&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The final variant calls will be present at &lt;code class=&quot;language-text&quot;&gt;deepvariant/deepvariant.vcf.gz&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;Evaluation&lt;/h3&gt;
&lt;p&gt;The veracity of the variant calling performed above can be obtained by
comparing the results to the &lt;a href=&quot;https://www.nist.gov/programs-projects/genome-bottle&quot;&gt;Genome In A
Bottle&lt;/a&gt; truth sets for
the GM24385 sample. The truth sets can be downloaded from the
&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/&quot;&gt;NCBI&lt;/a&gt; repository:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;for ext in .bed .bed.gz .bed.gz.tbi .vcf.gz .vcf.gz.tbi; do
    wget https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.1/GRCh38/HG002_GRCh38_1_22_v4.1_draft_benchmark${ext}
done&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;With these reference data we will use
&lt;a href=&quot;https://github.com/Illumina/hap.py&quot;&gt;hap.py&lt;/a&gt; to assess the recall and precision
of the variant calls made by DeepVariant. Hap.py is most easily run using the
docker container provided by it’s authors:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;docker run -it -v ${PWD}:${PWD} pkrusche/hap.py /opt/hap.py/bin/hap.py \
    ${PWD}/HG002_GRCh38_1_22_v4.1_draft_benchmark.vcf.gz ${PWD}/deepvariant/deepvariant.vcf.gz \
    -f ${PWD}/HG002_GRCh38_1_22_v4.1_draft_benchmark.bed \
    -r ${PWD}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta \
    -o happy_out --pass-only -l chr20 --engine=vcfeval --threads=20&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The output of the above will be a table summarising the results of the
comparison. An abbreviated form is given below:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;INDEL&lt;/th&gt;
&lt;th&gt;SNP&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;TRUTH.TOTAL&lt;/td&gt;
&lt;td&gt;11271&lt;/td&gt;
&lt;td&gt;71334&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;METRIC.Recall&lt;/td&gt;
&lt;td&gt;0.5927&lt;/td&gt;
&lt;td&gt;0.9972&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;METRIC.Precision&lt;/td&gt;
&lt;td&gt;0.8384&lt;/td&gt;
&lt;td&gt;0.9973&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;METRIC.F1_Score&lt;/td&gt;
&lt;td&gt;0.6944&lt;/td&gt;
&lt;td&gt;0.9973&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;An active area of research is the calling of variants in low complexity regions.
The NCBI reference data includes an index of such regions, we can mask these
regions from the comparison:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;wget https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/genome-stratifications/v2.0/GRCh38/LowComplexity/GRCh38_notinAllTandemRepeatsandHomopolymers_slop5.bed.gz
bedtools intersect \
    -a HG002_GRCh38_1_22_v4.1_draft_benchmark.bed \
    -b GRCh38_notinAllTandemRepeatsandHomopolymers_slop5.bed.gz 
    &amp;gt; HG002_GRCh38_1_22_v4.1_draft_benchmark.norepeat.bed

docker run -it -v ${PWD}:${PWD} pkrusche/hap.py /opt/hap.py/bin/hap.py \
    ${PWD}/HG002_GRCh38_GIABv4.1.vcf.gz ${PWD}/deepvariant/deepvariant.vcf.gz \
    -f ${PWD}/ HG002_GRCh38_1_22_v4.1_draft_benchmark.norepeat.bed \
    -r ${PWD}/truths/GRCh38_no_alt_chr20.fa \
    -o happy_out --pass-only -l chr20 --engine=vcfeval --threads=20&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;With these regions masked we now obtain:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;INDEL&lt;/th&gt;
&lt;th&gt;SNP&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;METRIC.Recall&lt;/td&gt;
&lt;td&gt;0.9458&lt;/td&gt;
&lt;td&gt;0.9992&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;METRIC.Precision&lt;/td&gt;
&lt;td&gt;0.9827&lt;/td&gt;
&lt;td&gt;0.9992&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;METRIC.F1_Score&lt;/td&gt;
&lt;td&gt;0.9639&lt;/td&gt;
&lt;td&gt;0.9992&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The latest research basecallers are better able to provide accurate calls
through low complexity regions. The example below compares basecalls produced
with Guppy 4.0.11 and the soon to be release Bonito version 0.3.0 basecaller.&lt;/p&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/ont-open-datasets/static/8664b21a426197e8eec64f62a26df624/46e51/bonito_igv.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 36.486486486486484%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsSAAALEgHS3X78AAABiklEQVQozz2R/W+aUBiF+f//G7O2fNmudUptARVwW2YAFUFcmkxFvmyXPHuhyX54cs6597w3ufcqbdtyvV4py4rDIadpSo7HP+R5RlUVwqXXui4pzkdZT6Vb9Gtvbweq+kJRnPr+5XJCyfOc/T4ny1KCIMIPMnw/xJ3F4lMhwfN2eH7C3NvgunHvfaHX4HPPna1FdyhVVdE2Le37la0dEA+e2GgTNuqYcPDIr5spkTZl9vCN7/cTIvOZULNYGRNi1RImrEXXMhOJV47HI01d08iBibMguRuRmBaJPmarjtjqFhvJ8Veb+NElkYMSXTqGxbbrSd52SD82RyhN01BXNaW81Q97haMGzMwlMyNgri1EPVzDZzqcYz0EuNoc1/RwjEXf6XCNJa7ezSxRzuczl+LC+98P1vZPwtsXIsMh1F/Fj+W6jmB/ZtGV+kykv8iV5Sl0h1gIVee/V06nk/zemaIsSe1X9uoNvw2VvXbL/m7AztBITck9d6T3GpunIZnkTHJHKnR7yfAL/wA/buqVykjzHwAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;bonito_low_complexity_calls&quot;
        title=&quot;Bonito Low Complexity Basecalls&quot;
        src=&quot;/ont-open-datasets/static/8664b21a426197e8eec64f62a26df624/fcda8/bonito_igv.png&quot;
        srcset=&quot;/ont-open-datasets/static/8664b21a426197e8eec64f62a26df624/12f09/bonito_igv.png 148w,
/ont-open-datasets/static/8664b21a426197e8eec64f62a26df624/e4a3f/bonito_igv.png 295w,
/ont-open-datasets/static/8664b21a426197e8eec64f62a26df624/fcda8/bonito_igv.png 590w,
/ont-open-datasets/static/8664b21a426197e8eec64f62a26df624/efc66/bonito_igv.png 885w,
/ont-open-datasets/static/8664b21a426197e8eec64f62a26df624/46e51/bonito_igv.png 1003w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;a href=&quot;http://software.broadinstitute.org/software/igv/&quot;&gt;IGV&lt;/a&gt; screenshot shows
how the Bonito basecaller is less prone to long deletion tracks in the low
complexity region. With improvements to basecalling we therefore anticipate
being able to produce highly accurate INDEL calls in these regions in the near
future.&lt;/p&gt;
&lt;h3&gt;Acknowledgements&lt;/h3&gt;
&lt;p&gt;We kindly acknowledge Andrew Carroll of Google Health and Benedict Paten’s
group at the Computational Genomics Lab, UC Santa Cruz Genomics Institute for
releasing the DeepVariant inference models for nanopore data and discussions
concerning how to present variant candidates to DeepVariant.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Katuali analysis pipeline for preparing human datasets]]></title><description><![CDATA[Our recent GM24385 data release contains data from multiple
flowcells and analytes for both the R9.4.1 and R10.3 flowcell chemistries. The…]]></description><link>https://nanoporetech.github.io/ont-open-datasets/katuali_human_pipeline/</link><guid isPermaLink="false">https://nanoporetech.github.io/ont-open-datasets/katuali_human_pipeline/</guid><pubDate>Tue, 22 Sep 2020 00:04:00 GMT</pubDate><content:encoded>&lt;p&gt;Our recent &lt;a href=&quot;/ont-open-datasets/gm24385_2020.09&quot;&gt;GM24385 data release&lt;/a&gt; contains data from multiple
flowcells and analytes for both the R9.4.1 and R10.3 flowcell chemistries. The
uploaded data contains the primary sequencer output data; the full MinKNOW
output directory for the runs is included verbatim. The release seperately
contains a directory structure resulting from the application of a snakemake
analysis pipeline. Here we provide details of how this workflow was executed
and its outputs.&lt;/p&gt;
&lt;h3&gt;Background&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/nanoporetech/katuali&quot;&gt;Katuali&lt;/a&gt; is a set of
&lt;a href=&quot;https://snakemake.readthedocs.io/&quot;&gt;Snakemake&lt;/a&gt; analysis pipelines for basic
analysis of nanopore sequencing data. It can perform basic tasks such as
basecalling, alignment of reads, assembly, and evaluation and benchmarking of
such algorithms. This can be performed at scale on large compute clusters on
local or cloud infrastructure.&lt;/p&gt;
&lt;p&gt;For the GM24385 release Katuali was used to construct secondary analyses in a
documented and reproducible fashion. As katuali is open source, it is possible
for users to reconstruct these secondary analyses for themselves from the
primary data. We have uploaded the results of these analysis to provide
benchmarking data and make available useful resources for others to perform
further analysis.&lt;/p&gt;
&lt;p&gt;The Katuali pipeline used for the &lt;a href=&quot;/ont-open-datasets/gm24385_2020.09&quot;&gt;GM24385 data release&lt;/a&gt;
provides four main outputs:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Align basecalls to reference sequence retaining all primary, secondary and
supplementary alignments are kept&lt;/li&gt;
&lt;li&gt;Filter .bam file to list of regions defined in configuration file retaining
only primary alignments.&lt;/li&gt;
&lt;li&gt;Produce read statistics from per-region .bams.&lt;/li&gt;
&lt;li&gt;Repack/group source .fast5 files according to primary alignment .bams to
produce per-region .fast5 file sets.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These outputs provide added value to the primary data, and users can extend and
adapt the katuali pipeline and configuration to calculate additional outputs.&lt;/p&gt;
&lt;h3&gt;Katuali configuration for GM24385 release&lt;/h3&gt;
&lt;p&gt;Katuali builds on native Snakemake functionality to provide a way of mapping
and analysis pipeline across multiple inputs with minimal fuss. How this is
achieved is describe in the katuali &lt;a href=&quot;https://nanoporetech.github.io/katuali/examples.html#automatic-generation-of-custom-pipeline-targets&quot;&gt;documentation&lt;/a&gt;. This functionality can be used to simulataneously
process data from multiple flowcells.&lt;/p&gt;
&lt;p&gt;A single configuration file is used to control Katuali’s behaviour: what input
data it will use, what pipelines it will run, and the configuration of external
programs that it runs. The configuration file can be created from the
provided template using the &lt;a href=&quot;https://nanoporetech.github.io/katuali/tests.html#predefined-workflows&quot;&gt;katuali_config&lt;/a&gt;
command.&lt;/p&gt;
&lt;p&gt;For the purposes of the GM24385 data release this file was then customised with
details of the input datasets (the &lt;code class=&quot;language-text&quot;&gt;.fast5&lt;/code&gt;/&lt;code class=&quot;language-text&quot;&gt;.fastq&lt;/code&gt; files from MinKNOW) and a
description of the outputs that were required. The resulting files are included
in the data release under the &lt;code class=&quot;language-text&quot;&gt;config&lt;/code&gt; folder at:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;s3://ont-open-data/gm24385_2020.09/config/&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;See our &lt;a href=&quot;/ont-open-datasets/tutorials/&quot;&gt;tutorials&lt;/a&gt; page for details on how to download these
files.&lt;/p&gt;
&lt;h4&gt;Setup of input directories&lt;/h4&gt;
&lt;p&gt;Katuali can be used to perform basecalling from &lt;code class=&quot;language-text&quot;&gt;.fast5&lt;/code&gt; files to produce
standard &lt;code class=&quot;language-text&quot;&gt;.fastq&lt;/code&gt; sequence files. However since basecalling was performed
during the sequencing experiments we can sidestep the basecalling procedure
and simply bootstrap the Katuali output directory with the already computed
basecalls. To do this the &lt;code class=&quot;language-text&quot;&gt;setup_katuali.sh&lt;/code&gt; program, located at:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;s3://ont-open-data/gm24385_2020.09/config/setup_katuali.sh&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;was used. This prepares a directory structure that Katuali would otherwise
produce itself whilst avoiding some expensive computations. Seperate top-level
Katuali directories were created to group flowcell data from R9.4.1 and R10.3
flowcells.&lt;/p&gt;
&lt;h4&gt;Important aspects of configuration file&lt;/h4&gt;
&lt;p&gt;The template configuration files need only minor customisation for the GM24385
dataset. Firstly the &lt;code class=&quot;language-text&quot;&gt;DATA:&lt;/code&gt; section requires specifying, for example the
R9.4.1 file contains an entries such as the following (one per flowcell):&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;DATA:
    &amp;#39;20200914_1356_6F_PAF26223_da14221a&amp;#39;:
        &amp;#39;REFERENCE&amp;#39;: &amp;#39;ref/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta&amp;#39;
        &amp;#39;SPLIT_FAST5_REGIONS&amp;#39;:
            [&amp;#39;chr1&amp;#39;, &amp;#39;chr2&amp;#39;, &amp;#39;chr3&amp;#39;, &amp;#39;chr4&amp;#39;, &amp;#39;chr5&amp;#39;, &amp;#39;chr6&amp;#39;, &amp;#39;chr7&amp;#39;, &amp;#39;chr8&amp;#39;, &amp;#39;chr9&amp;#39;, &amp;#39;chr10&amp;#39;,
            &amp;#39;chr11&amp;#39;, &amp;#39;chr12&amp;#39;, &amp;#39;chr13&amp;#39;, &amp;#39;chr14&amp;#39;, &amp;#39;chr15&amp;#39;, &amp;#39;chr16&amp;#39;, &amp;#39;chr17&amp;#39;, &amp;#39;chr18&amp;#39;, &amp;#39;chr19&amp;#39;, &amp;#39;chr20&amp;#39;,
            &amp;#39;chr21&amp;#39;, &amp;#39;chr22&amp;#39;, &amp;#39;chrX&amp;#39;, &amp;#39;chrY&amp;#39;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The first item here is simply the name MinKNOW output directory. The
&lt;code class=&quot;language-text&quot;&gt;REFERENCE&lt;/code&gt; entry is a relative filepath to the genomic reference sequence
appropriate to the sample; in the case of the GM24385 dataset this is simply
the human reference genome. The final entry in the above is a list of sequence
identifies (corresponding to entries in the &lt;code class=&quot;language-text&quot;&gt;REFERENCE&lt;/code&gt; file) indicating how
the dataset should be separated after alignment of sequences to the reference.&lt;/p&gt;
&lt;p&gt;The second important customisation of the template configuration file is the specification
of which output files should be created. This appears in the &lt;code class=&quot;language-text&quot;&gt;PIPELINES&lt;/code&gt; section:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;PIPELINES:
    all_initial: [
        # do alignments, split bams and fast5s by regions
        &amp;quot;{DATA}/guppy_v4.0.11_r10.3_hac_prom/align_unfiltered/{SPLIT_FAST5_REGIONS}/fast5/&amp;quot;,
    ]
    all_add_alignment_stats: [
        # calculate alignment stats
        &amp;quot;{DATA}/guppy_v4.0.11_r10.3_hac_prom/align_unfiltered/calls2ref_stats.txt&amp;quot;,
        &amp;quot;{DATA}/guppy_v4.0.11_r10.3_hac_prom/align_unfiltered/{SPLIT_FAST5_REGIONS}/calls2ref_stats.txt&amp;quot;
    ]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;What these so-called “targets” cause Katuali and Snakemake to calculate is discussed below.
To have katuali perform the calculation for all items in the &lt;code class=&quot;language-text&quot;&gt;DATA&lt;/code&gt; section it is sufficient
to run katuali with, for example:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;katuali all_initial --configfile ../../config/r9.4.1.config&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;from the directory corresponding to:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;s3://ont-open-data/gm24385_2020.09/analysis/r9.4.1/&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Katuali replaces the &lt;code class=&quot;language-text&quot;&gt;{DATA}&lt;/code&gt; and &lt;code class=&quot;language-text&quot;&gt;{SPLIT_FAST5_REGIONS}&lt;/code&gt; tags in the listings above with
all possible values listed in the &lt;code class=&quot;language-text&quot;&gt;DATA&lt;/code&gt; section of the configuration file. The resulting
matrix of targets is given to Snakemake to perform the workflows.&lt;/p&gt;
&lt;h3&gt;Pipeline data flow and output descriptions&lt;/h3&gt;
&lt;p&gt;The filepath targets defined in the Katuali configuration files trigger
Snakemake to perform all necessary calculations required to produce the
requested files. Users interested in the Snakemake rules used to produce all
files should consult the Katuali documentation and source files
&lt;a href=&quot;https://github.com/nanoporetech/katuali/tree/master/katuali/data&quot;&gt;here&lt;/a&gt;, which
are grouped logically according to function.&lt;/p&gt;
&lt;p&gt;Katuali has for the most part a convention that outputs which are derived
directly from an input (or previous intermediate output) are stored in a
sub-directory of that previous input. This leads to a continued deepening of
the directory structure. A benefit of this approach is the ability to
recursively calculate new outputs of the same type without having to write
multiple rules. Downside of the approach are that it is not always easy to see
which items are the immediate outputs of an analysis stage and which are the
outputs of subsequent stages. To aid users who do not wish to examine the
Snakemake files included in Katuali, the directory listing below will aid
comprehension.&lt;/p&gt;
&lt;p&gt;Katuali generically labels results of analysis stages using a
&lt;code class=&quot;language-text&quot;&gt;&amp;lt;stage&amp;gt;_&amp;lt;suffix&amp;gt;&lt;/code&gt; form, for example in the below the top level
&lt;code class=&quot;language-text&quot;&gt;&amp;lt;guppy_v4.0.11_r10.3_hac_prom&amp;gt;&lt;/code&gt; indicates results under this level
are results of the Guppy basecaller using the settings specified
in the Katuali configuration file under the tag &lt;code class=&quot;language-text&quot;&gt;v4.0.11_r10.3_hac_prom&lt;/code&gt;.
Similarly &lt;code class=&quot;language-text&quot;&gt;&amp;lt;align_unfiltered&amp;gt;&lt;/code&gt; indicates results from the alignment
rule generated using the &lt;code class=&quot;language-text&quot;&gt;unfiltered&lt;/code&gt; settings.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;.
├── guppy_v4.0.11_r10.3_hac_prom
│   ├── basecalls.fastq
│   ├── sequencing_summary.txt
│   ├── align_unfiltered
│   │   ├── calls2ref.bam
│   │   ├── calls2ref.bam.bai
│   │   ├── calls2ref_stats.txt
│   │   ├── chr1
│   │   │   ├── calls2ref.bam
│   │   │   ├── calls2ref.bam.bai
│   │   │   ├── calls2ref_stats.txt
│   │   │   ├── fast5
│   │   │   │   ├── batch0.fast5
│   │   │   │   ├── batch1.fast5
│   │   │   │   ├── ...
│   │   │   │   └── filename_mapping.txt
│   │   │   └── readlist.txt
│   │   ├── chr2
┊	┊   ┊	...
└── reads -&amp;gt; &amp;lt;link to MinKNOW fast5_pass directory&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;(Log files have been omitted from the above listing).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The guppy analysis stage&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Guppy analysis stage has two outputs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;basecalls.fastq&lt;/code&gt; - all basecalls from the basecaller in a single file.&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;sequencing_summary.txt&lt;/code&gt; - per-read summary information (as produced by MinKNOW).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Alignment analysis stage&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The alignment stage of the workflow produced the following files under the
&lt;code class=&quot;language-text&quot;&gt;align_unfiltered&lt;/code&gt; directory. The &lt;code class=&quot;language-text&quot;&gt;unfiltered&lt;/code&gt; suffix relates to the fact that
all alignments are retained, not simply the primary alignment of each read.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;calls2ref.bam&lt;/code&gt; - the alignments of reads to the supplied reference.&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;calls2ref.bam.bai&lt;/code&gt; - an index file for the alignments.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;An auxiliary target of the Katuali pipeline produces the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;calls2ref_stats.txt&lt;/code&gt; - per-read statistics calculated from the corresponding
&lt;code class=&quot;language-text&quot;&gt;calls2ref.bam&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Alignment filtering stage&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Having aligned the basecall data, Katuali separates the basecalls and read data
stored in the source &lt;code class=&quot;language-text&quot;&gt;.fast5&lt;/code&gt; data by the regions specified in the Katuali
config. A directory is produced by region, for example &lt;code class=&quot;language-text&quot;&gt;chr1&lt;/code&gt; in the listing
above. Under this we find:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;calls2ref*&lt;/code&gt; - files analagous to those in the &lt;code class=&quot;language-text&quot;&gt;align_unfiltered&lt;/code&gt; directory
but containing only those reads with primary alignments to the given region.&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;readlist.txt&lt;/code&gt; - a simple text table containing the read identifiers of the
requisite reads.&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;fast5&lt;/code&gt; - a directory containing &lt;code class=&quot;language-text&quot;&gt;.fast5&lt;/code&gt; files constructed from the original
MinKNOW &lt;code class=&quot;language-text&quot;&gt;.fast5&lt;/code&gt; files but containing only the requisite reads. The file
&lt;code class=&quot;language-text&quot;&gt;filename_mapping.txt&lt;/code&gt; provides a read identifier to filename mapping.&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[GM24385 Dataset Release]]></title><description><![CDATA[We are happy to annouce the release of a Nanopore sequencing dataset
of the Genome in a Bottle sample GM24385. Multiple PromethION flowcells…]]></description><link>https://nanoporetech.github.io/ont-open-datasets/gm24385_2020.09/</link><guid isPermaLink="false">https://nanoporetech.github.io/ont-open-datasets/gm24385_2020.09/</guid><pubDate>Tue, 22 Sep 2020 00:03:00 GMT</pubDate><content:encoded>&lt;p&gt;We are happy to annouce the release of a Nanopore sequencing dataset
of the Genome in a Bottle sample GM24385.&lt;/p&gt;
&lt;p&gt;Multiple PromethION flowcells using both the R9.4.1 and R10.3 nanopores.
The direct sequencer output is included, raw signal data stored in
.fast5 files and basecalled data in .fastq file. Additional secondary
analyses are included, notably alignments of sequence data to the
reference genome are provided along with statistics derived from these.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Whats included?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The dataset comprises multiple R9.4.1 and R10.3 flowcells of multiple
separately prepared samples; each sample was run on each flowcell type.&lt;/p&gt;
&lt;p&gt;The initial sequencer outputs are included in self container directories.
In addition derived outputs from an automated pipeline are stored
separately.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Details concerning sample preparations&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Below is described briefly the method of analyte preparation. Standard, published
protocols were followed with no intentional deviation.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The following cell line samples were obtained from the NIGMS Human Genetic Cell
Repository at the Coriell Institute for Medical Research: GM24385&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;High molecular weight DNA from GM24385 lymphoblastoid cells was prepared by
&lt;a href=&quot;https://www.evotec.com/en&quot;&gt;Evotec&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.circulomics.com/store/Short-Read-Eliminator-XL-p138401730&quot;&gt;Circulomics Short Read Eliminator XL&lt;/a&gt;
protocol was used to deplete DNA fragments &amp;#x3C; 40kb in length.&lt;/li&gt;
&lt;li&gt;DNA was end repaired and dA tailed prior to LSK based library preparation.&lt;/li&gt;
&lt;li&gt;DNA sequencing was performed using PromethION device.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The dataset comprises multiple flowcells for each pore:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&quot;center&quot;&gt;Pore&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;Treatmeant&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;Flowcells&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot;&gt;R9.4.1&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;SRE&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;PAF27096, PAF27462&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot;&gt;R10.3&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;SRE&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;PAF26223, PAF26161&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Location of the data&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The data is located in an Amazon Webservice S3 bucket at:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;s3://ont-open-data/gm24385_2020.09/&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;See the &lt;a href=&quot;/ont-open-datasets/tutorials/&quot;&gt;tutorials&lt;/a&gt; page for information on downloading the dataset.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Description of available data&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The uploaded dataset has been prepared using a snakemake pipeline to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Align basecalls to reference sequence. All primary, secondary and
supplementary alignments are kept&lt;/li&gt;
&lt;li&gt;Filter .bam file to list of regions defined in configuration file
retaining only primary alignments.&lt;/li&gt;
&lt;li&gt;Produce read statistics from per-region .bams.&lt;/li&gt;
&lt;li&gt;Repack/group source .fast5 files according to primary alignment .bams
to produce per-region .fast5 file sets.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For more details see our &lt;a href=&quot;/ont-open-datasets/katuali_human_pipeline/&quot;&gt;post&lt;/a&gt; detailing the
pipeline and its outputs.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[ONT Open Datasets Tutorials]]></title><description><![CDATA[Below you will find helpful information and links out to tutorials for
datasets within the Open Datasets project. How can I access the data…]]></description><link>https://nanoporetech.github.io/ont-open-datasets/tutorials/</link><guid isPermaLink="false">https://nanoporetech.github.io/ont-open-datasets/tutorials/</guid><pubDate>Tue, 22 Sep 2020 00:01:00 GMT</pubDate><content:encoded>&lt;p&gt;Below you will find helpful information and links out to tutorials for
datasets within the Open Datasets project.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;How can I access the data?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;All the data is stored under in an Amazon Web Services S3 bucket:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;s3://ont-open-data&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This public has public permissions for anyone to obtain the data
without requiring login credentials. To download the data we
recommend using the &lt;a href=&quot;https://aws.amazon.com/cli/&quot;&gt;AWS command line interface&lt;/a&gt;.
With the CLI installed listing the datasets can be performed with:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;aws s3 ls --no-sign-request s3://ont-open-data/&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;There will be a subdirectory per dataset release. Inside each dataset
will be contained a README.md file with brief details. This website
will contain additional details for each dataset.&lt;/p&gt;
&lt;p&gt;To download datasets or extracts thereof we recommend using the &lt;code class=&quot;language-text&quot;&gt;sync&lt;/code&gt;
command:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;aws s3 sync --no-sign-request s3://ont-open-data/gm24385_2020.09 gm24385_2020.09&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Datasets may be added to or amended over time so using &lt;code class=&quot;language-text&quot;&gt;sync&lt;/code&gt; can be
used with a previously fetch copy to update with the latest changes
and additions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Running workflows using ONT’s EPI2ME platform&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Oxford Nanopore’s &lt;a href=&quot;https://epi2me.nanoporetech.com/&quot;&gt;EPI2ME&lt;/a&gt;
platform includes several workflows for analysing
genomics datasets in the cloud with Amazon Web Services, reducing the
computational burden on the users local compute infrastructure.
For example, the &lt;a href=&quot;https://community.nanoporetech.com/protocols/epi2me/v/mte_1014_v1_revba_11apr2016/human-alignment-grch38&quot;&gt;FASTQ Human Alignment GRCh38&lt;/a&gt;
can be used to perform alignment of nanopore sequencing reads to
the human reference genome to obtain industry standard BAM file outputs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;How can I recreate the analysis directories?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;With each dataset release we will include details of how the analysis
directories were constructed from the primary inputs. For example for
the &lt;a href=&quot;/ont-open-datasets/gm24385_2020.09&quot;&gt;GM24385&lt;/a&gt; dataset release &lt;a href=&quot;/ont-open-datasets/katuali_human_pipeline&quot;&gt;this page&lt;/a&gt;
contains details of the Snakemake pipeline used.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[ONT Open Datasets Launch]]></title><description><![CDATA[The ont-open-data registry provides reference sequencing data from Oxford
Nanopore Technologies to support, Exploration of the…]]></description><link>https://nanoporetech.github.io/ont-open-datasets/launch_annouce/</link><guid isPermaLink="false">https://nanoporetech.github.io/ont-open-datasets/launch_annouce/</guid><pubDate>Tue, 22 Sep 2020 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;The ont-open-data registry provides reference sequencing data from Oxford
Nanopore Technologies to support,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Exploration of the characteristics of nanopore sequence data,&lt;/li&gt;
&lt;li&gt;Assessment and reproduction of performance benchmarks,&lt;/li&gt;
&lt;li&gt;Example datasets for analysis on EPI2ME, Oxford Nanopore Technologies’ cloud
compute infrastructure,&lt;/li&gt;
&lt;li&gt;Development of tools and methods.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The data deposited showcases DNA sequences from a representative subset of
sequencing chemistries. The datasets correspond to publicly-available reference
samples (e.g. GM24385 as reference human). Raw data are provided with metadata
and scripts to describe sample and data provenance.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[About]]></title><description><![CDATA[The ont-open-data registry provides reference sequencing data from Oxford
Nanopore Technologies to support, Exploration of the…]]></description><link>https://nanoporetech.github.io/ont-open-datasets/</link><guid isPermaLink="false">https://nanoporetech.github.io/ont-open-datasets/</guid><pubDate>Tue, 22 Sep 2020 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;The ont-open-data registry provides reference sequencing data from Oxford
Nanopore Technologies to support,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Exploration of the characteristics of nanopore sequence data,&lt;/li&gt;
&lt;li&gt;Assessment and reproduction of performance benchmarks,&lt;/li&gt;
&lt;li&gt;Example datasets for analysis on EPI2ME, Oxford Nanopore Technologies’ cloud
compute infrastructure,&lt;/li&gt;
&lt;li&gt;Development of tools and methods.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The data deposited showcases DNA sequences from a representative subset of
sequencing chemistries. The datasets correspond to publicly-available reference
samples (e.g. GM24385 as reference human). Raw data are provided with metadata
and scripts to describe sample and data provenance.&lt;/p&gt;</content:encoded></item></channel></rss>